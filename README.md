# AI Security Assurance Labs

This repository showcases my hands-on work in **AI Security Assurance**, including:

- Model supply-chain verification
- LLM red teaming and evaluation
- Static analysis of AI model artifacts
- Alignment with **NIST AI RMF** and **MITRE ATLAS**
- Scripts and processes for repeatable AI model intake and testing

The goal of this lab is to simulate how an enterprise AI security function can evaluate and approve models before they are used in production.

## Repository Structure

- `model-supply-chain/` – Model intake, integrity checks, PyRIT-style workflows, supply-chain validation.
- `red-teaming/` – LLM red teaming using Garak and Promptfoo, plus example prompts and findings.
- `static-analysis/` – YARA and ClamAV usage for scanning model files and related artifacts.
- `frameworks/` – Mapping lab work to NIST AI RMF, MITRE ATLAS, and other security frameworks.
- `scripts/` – PowerShell and other helper scripts (hashing, pipeline orchestration, etc.).
- `reports/` – Example AI security assurance reports and risk summaries.
- `docs/` – Methodology, terminology, and notes.

> This is a personal learning and demonstration lab. No proprietary or confidential data is used here.
