# LLM Red Teaming

This section contains examples of **red teaming AI language models**, including:

- Automated vulnerability scanning using Garak  
- Scenario-based adversarial testing using Promptfoo  
- Manual prompt-based stress testing  
- Evaluation of jailbreaks, prompt injection, refusal bypasses, and unsafe outputs  
- Interpretation of findings and mapping to AI security frameworks  

Subdirectories:

- `garak/` – Garak configs, results, and sample assessments  
- `promptfoo/` – Promptfoo red-team configurations and example evaluation setups
