# LLM Red Teaming

This section contains materials related to red teaming large language models (LLMs). It includes automated, scenario-based, and manual adversarial testing methods used to evaluate model robustness, safety alignment, and resistance to harmful or unintended behavior.

Red-teaming activities typically occur after model intake and supply-chain validation and inform downstream risk classification and approval decisions.

Key focus areas include:

- Automated vulnerability scanning using Garak  
- Scenario-driven adversarial evaluations using Promptfoo  
- Manual prompt-based stress testing  
- Assessment of jailbreaks, prompt injection, refusal bypasses, and unsafe outputs  
- Interpretation of red-team findings within AI security frameworks  

Subdirectories:

- `garak/` — Garak configurations, outputs (synthetic examples only), and assessment resources  
- `promptfoo/` — Promptfoo configuration files and red-team evaluation setups
